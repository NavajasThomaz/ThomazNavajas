services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: thomaz-navajas-app
    # Não expor porta externamente - apenas para cloudflared
    expose:
      - "3000"
    environment:
      - NODE_ENV=production
      - ABACUS_API_URL=${ABACUS_API_URL:-https://apps.abacus.ai}
      - ABACUS_DEPLOYMENT_TOKEN=${ABACUS_DEPLOYMENT_TOKEN}
      - ABACUS_DEPLOYMENT_ID=${ABACUS_DEPLOYMENT_ID}
      - ABACUS_API_KEY=${ABACUS_API_KEY}
      - ABACUS_CONVERSATION_ID=${ABACUS_CONVERSATION_ID}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - app-network

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: thomaz-navajas-tunnel
    command: tunnel --config /tmp/cloudflared-config.yml run
    volumes:
      # Arquivo de configuração do tunnel
      - ./cloudflared-config.yml:/tmp/cloudflared-config.yml:ro
      # Credenciais do tunnel (geradas após login)
      - ${CLOUDFLARE_CREDENTIALS_PATH:-./.cloudflared}/:/etc/cloudflared/:ro
    restart: unless-stopped
    depends_on:
      - app
    networks:
      - app-network

  backend:
    build:
      context: ./backend
    container_name: thomaz-navajas-backend
    expose:
      - "8000"
    environment:
      - DEFAULT_MODEL=llama3
    restart: unless-stopped
    networks:
      - app-network

  ollama:
    image: ollama/ollama:latest
    container_name: thomaz-navajas-ollama
    expose:
      - "11434"
    restart: unless-stopped
    runtime: nvidia
    environment:
      - OLLAMA_KEEP_ALIVE=10m
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NUM_PARALLEL=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  ollama-data:
